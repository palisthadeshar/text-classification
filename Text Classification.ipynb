{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3439f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Conv1D, SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1D, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad532b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link   \n",
       "0  https://www.huffpost.com/entry/covid-boosters-...  \\\n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category   \n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS  \\\n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors   \n",
       "0  Health experts said it is too early to predict...  Carla K. Johnson, AP  \\\n",
       "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
       "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
       "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
       "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
       "\n",
       "        date  \n",
       "0 2022-09-23  \n",
       "1 2022-09-23  \n",
       "2 2022-09-23  \n",
       "3 2022-09-23  \n",
       "4 2022-09-22  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file into a pandas DataFrame\n",
    "data = pd.read_json('./data/News_Category_Dataset_v3.json',lines=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e204adbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline   category   \n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS  \\\n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description  \n",
       "0  Health experts said it is too early to predict...  \n",
       "1  He was subdued by passengers and crew when he ...  \n",
       "2  \"Until you have a dog you don't understand wha...  \n",
       "3  \"Accidentally put grown-up toothpaste on my to...  \n",
       "4  Amy Cooper accused investment firm Franklin Te...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #dropping unnecessary columns\n",
    "new_df = data.drop(columns=['authors','link','date'])\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3fe10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13aa8dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>length_of_news</th>\n",
       "      <th>len_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                     length_of_news  len_news\n",
       "0  U.S. NEWS  Over 4 Million Americans Roll Up Sleeves For O...       230\n",
       "1  U.S. NEWS  American Airlines Flyer Charged, Banned For Li...       248\n",
       "2     COMEDY  23 Of The Funniest Tweets About Cats And Dogs ...       133\n",
       "3  PARENTING  The Funniest Tweets From Parents This Week (Se...       215\n",
       "4  U.S. NEWS  Woman Who Called Cops On Black Bird-Watcher Lo...       233"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create final dataframe of combined headline and short_description\n",
    "final_df = new_df.copy()\n",
    "final_df['length_of_news'] = final_df['headline'] + final_df['short_description']\n",
    "final_df.drop(['headline','short_description'], inplace=True, axis=1)\n",
    "final_df['len_news'] = final_df['length_of_news'].apply(lambda x: len(str(x)))\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bc78115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>length_of_news</th>\n",
       "      <th>len_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                     length_of_news  len_news\n",
       "0  U.S. NEWS  Over 4 Million Americans Roll Up Sleeves For O...       230\n",
       "1  U.S. NEWS  American Airlines Flyer Charged, Banned For Li...       248\n",
       "2     COMEDY  23 Of The Funniest Tweets About Cats And Dogs ...       133\n",
       "3  PARENTING  The Funniest Tweets From Parents This Week (Se...       215\n",
       "4  U.S. NEWS  Woman Who Called Cops On Black Bird-Watcher Lo...       233"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_df.drop(columns=['category'])\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c5311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "    # Join the words\n",
    "    preprocessed_text = ' '.join(words)\n",
    "    return preprocessed_text\n",
    "\n",
    "final_df['length_of_news'] = final_df['length_of_news'].apply(lambda x: preprocess_text(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b370bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['category_merged']=final_df['category'].replace({\"HEALTHY LIVING\": \"WELLNESS\",\n",
    "\"QUEER VOICES\": \"GROUPS VOICES\",\n",
    "\"BUSINESS\": \"BUSINESS & FINANCES\",\n",
    "\"PARENTS\": \"PARENTING\",\n",
    "\"BLACK VOICES\": \"GROUPS VOICES\",\n",
    "\"THE WORLDPOST\": \"WORLD NEWS\",\n",
    "\"STYLE\": \"STYLE & BEAUTY\",\n",
    "\"GREEN\": \"ENVIRONMENT\",\n",
    "\"TASTE\": \"FOOD & DRINK\",\n",
    "\"WORLDPOST\": \"WORLD NEWS\",\n",
    "\"SCIENCE\": \"SCIENCE & TECH\",\n",
    "\"TECH\": \"SCIENCE & TECH\",\n",
    "\"MONEY\": \"BUSINESS & FINANCES\",\n",
    "\"ARTS\": \"ARTS & CULTURE\",\n",
    "\"COLLEGE\": \"EDUCATION\",\n",
    "\"LATINO VOICES\": \"GROUPS VOICES\",\n",
    "\"CULTURE & ARTS\": \"ARTS & CULTURE\",\n",
    "\"FIFTY\": \"MISCELLANEOUS\",\n",
    "\"GOOD NEWS\": \"MISCELLANEOUS\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b131d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data:  (209527,)\n",
      "shape of target variable:  (209527,)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding using keras tokenizer and pad sequencing\n",
    "X = final_df['length_of_news']\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(final_df['category_merged'])\n",
    "print(\"shape of input data: \", X.shape)\n",
    "print(\"shape of target variable: \", y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# train_seq = tokenizer.texts_to_sequences(train_data)\n",
    "# test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c81887b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209527\n"
     ]
    }
   ],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76711ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167621, 150)\n"
     ]
    }
   ],
   "source": [
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=150) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "# padding X_test text input data\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "test_padseq = pad_sequences(test_seq, maxlen=150)\n",
    "print(train_padseq.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e8346e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of word index: 158715\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "max_words = 50000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index)\n",
    "maxlen = 150 # max length of sequence \n",
    "num_classes = len(final_df['category_merged'].unique())\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d99e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U.S. NEWS' 'COMEDY' 'PARENTING' 'WORLD NEWS' 'ARTS & CULTURE'\n",
      " 'SCIENCE & TECH' 'SPORTS' 'ENTERTAINMENT' 'POLITICS' 'WEIRD NEWS'\n",
      " 'ENVIRONMENT' 'EDUCATION' 'CRIME' 'WELLNESS' 'BUSINESS & FINANCES'\n",
      " 'STYLE & BEAUTY' 'FOOD & DRINK' 'MEDIA' 'GROUPS VOICES' 'HOME & LIVING'\n",
      " 'WOMEN' 'TRAVEL' 'RELIGION' 'IMPACT' 'WEDDINGS' 'MISCELLANEOUS' 'DIVORCE']\n"
     ]
    }
   ],
   "source": [
    "print(final_df['category_merged'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9acd6fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167621,)\n",
      "(167621, 27)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(y_train.shape)\n",
    "from tensorflow.keras.layers import Input\n",
    "# input_layer = Input(shape=(maxlen,))\n",
    "input_layer = Input(shape=(20,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b6c9681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 70)           3500000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 150, 128)         17280     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 150, 128)         24704     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 32)                5152      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 28)                924       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,548,060\n",
      "Trainable params: 3,548,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Reshape\n",
    "# basline model using embedding layers and simpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 70, input_length=maxlen))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.30, activation='tanh', return_sequences=True)))\n",
    "model.add(SimpleRNN(32, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(28, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e743e47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1048/1048 [==============================] - 590s 559ms/step - loss: 2.5400 - accuracy: 0.3423 - val_loss: 2.3420 - val_accuracy: 0.3769\n",
      "Epoch 2/10\n",
      "1048/1048 [==============================] - 722s 689ms/step - loss: 2.3054 - accuracy: 0.3949 - val_loss: 2.1756 - val_accuracy: 0.4263\n",
      "Epoch 3/10\n",
      "1048/1048 [==============================] - 747s 713ms/step - loss: 2.1824 - accuracy: 0.4278 - val_loss: 2.1309 - val_accuracy: 0.4291\n",
      "Epoch 4/10\n",
      "1048/1048 [==============================] - 713s 680ms/step - loss: 2.1053 - accuracy: 0.4446 - val_loss: 2.1203 - val_accuracy: 0.4437\n",
      "Epoch 5/10\n",
      "1048/1048 [==============================] - 846s 807ms/step - loss: 2.0549 - accuracy: 0.4593 - val_loss: 2.0312 - val_accuracy: 0.4667\n",
      "Epoch 6/10\n",
      "1048/1048 [==============================] - 799s 762ms/step - loss: 2.0163 - accuracy: 0.4713 - val_loss: 2.0028 - val_accuracy: 0.4723\n",
      "Epoch 7/10\n",
      "1048/1048 [==============================] - 763s 728ms/step - loss: 1.9943 - accuracy: 0.4799 - val_loss: 1.9654 - val_accuracy: 0.4894\n",
      "Epoch 8/10\n",
      "1048/1048 [==============================] - 743s 709ms/step - loss: 1.9827 - accuracy: 0.4857 - val_loss: 1.9833 - val_accuracy: 0.4893\n",
      "Epoch 9/10\n",
      "1048/1048 [==============================] - 791s 755ms/step - loss: 1.9718 - accuracy: 0.4887 - val_loss: 1.9953 - val_accuracy: 0.4863\n",
      "Epoch 10/10\n",
      "1048/1048 [==============================] - 621s 593ms/step - loss: 1.9526 - accuracy: 0.4963 - val_loss: 1.9310 - val_accuracy: 0.4988\n",
      "test loss and accuracy: 1.9307653903961182 0.49766144156455994\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue',monitor='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "# fit model to the data\n",
    "history = model.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=10, \n",
    "                     validation_split=0.2,\n",
    "                     shuffle=True\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss, test_acc = model.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss, test_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbf26baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 100)          15871500  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 150, 256)         234496    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 150, 256)         394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 150, 128)         41088     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 148, 72)           27720     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 74, 72)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 74, 64)            8768      \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                24960     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 27)                1755      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,604,527\n",
      "Trainable params: 16,604,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model_3.add(Bidirectional(LSTM(128, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model_3.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model_3.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model_3.add(Conv1D(72, 3, activation='relu'))\n",
    "model_3.add(MaxPooling1D(2))\n",
    "model_3.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model_3.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model_3.add(Dropout(0.2))\n",
    "model_3.add(Dense(27, activation='sigmoid'))\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b2732cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  12/1048 [..............................] - ETA: 15:59:51 - loss: 9.5152 - accuracy: 0.1348"
     ]
    }
   ],
   "source": [
    "model_3.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history3 = model_3.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=20, \n",
    "                     validation_split=0.2,\n",
    "                     shuffle=True\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss3, test_acc3 = model_3.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss3, test_acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b877a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traininggggggggggg....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f4f9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './models/model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b6f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import normalize\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1cb1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('./models/modelLastMayb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02f503a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data=\"Google has announced the release of its new Pixel 7 tablet, which is set to hit the market in 2023. The tablet is designed to be super fast and responsive, with an adaptive battery and a range of helpful features. It also boasts Pixel's best photo and video capabilities, making it a great choice for anyone looking for a high-quality tablet. The Pixel 7 tablet was unveiled at Google's fall event, where the company also showed off its Pixel 7 and Pixel 7 Pro phones\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "974ce9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=preprocess_text(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f04bfc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    google has announced the release of its new pi...\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.Series(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "720fdc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,\n",
       "        16, 17,  2, 18,  8,  9, 19,  3,  4, 20, 10, 21, 11, 22,  2, 23,\n",
       "        24,  2,  4, 10, 25, 11, 26, 27, 28,  5, 29, 30, 31, 32, 33,  5,\n",
       "         6, 34,  8, 35, 36, 12, 13, 37,  3, 14, 38, 39,  5, 40, 41, 42,\n",
       "        12,  6, 43, 44, 15, 45, 46, 15,  6, 47, 48,  4,  2,  3,  4, 49,\n",
       "        50, 51,  7, 14, 52, 53, 54,  2, 55, 13, 56, 57,  9,  3,  5,  3,\n",
       "        58, 59]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(data)\n",
    "train_seq = tokenizer.texts_to_sequences(data)\n",
    "train_padseq = pad_sequences(train_seq, maxlen=130)\n",
    "train_padseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6b572c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction=model.predict(train_padseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "417fdee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.2075366e-03 1.4590668e-03 2.5817575e-03 9.1638055e-04 6.9344585e-04\n",
      "  1.4415936e-02 1.9356011e-03 4.0651558e-04 1.0345387e-03 6.3660160e-02\n",
      "  1.4929048e-04 2.3221144e-01 7.4354769e-04 2.7101275e-02 1.4112805e-02\n",
      "  3.5614066e-02 3.0220391e-02 5.5948342e-04 5.0797616e-04 3.7299382e-04\n",
      "  1.6461028e-04 5.6560042e-05 4.3214939e-05 2.9646998e-04 1.9082578e-02\n",
      "  3.6450870e-02 1.8990554e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ed1303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "encoded_argmax  = np.argmax(prediction, axis=1)\n",
    "# text = tokenizer.sequences_to_texts([encoded_argmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98def1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_encoder = LabelEncoder()\n",
    "y = my_encoder.fit_transform(final_df['category_merged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9f589a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(my_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94ceca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encoder.pkl', 'rb') as f:\n",
    "    encode = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ac7c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = my_encoder.inverse_transform(encoded_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e781a529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMPACT']\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1769be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
